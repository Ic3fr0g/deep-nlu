{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "date-normalization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhugopinathan/deep-nlu/blob/master/date_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z15LePKIGhd8",
        "colab_type": "text"
      },
      "source": [
        "# Date Normalization with RNN Encoder / Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLYkl9A69r0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDuAnqSnGqEw",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujqm7frWEtBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"date-normalization\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  os.mkdir(DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSTUTlByFGZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIR = \"https://raw.githubusercontent.com/madhugopinathan/deep-nlu/master/data\"\n",
        "for fn in [\"training.csv\", \"validation.csv\", \"test.csv\"]:\n",
        "  df = pd.read_csv(f\"{DIR}/human-machine-date-{fn}\")\n",
        "  df.to_csv(f\"date-normalization/{fn}\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiEYXn0S92Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\", header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdnJWOLPEOqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cb7b37bd-ebd9-4b1b-fa38-107bfa7c0d17"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26, SEP 2007</td>\n",
              "      <td>2007-09-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27 September, 1986</td>\n",
              "      <td>1986-09-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24.02.82</td>\n",
              "      <td>1982-02-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>04-may-1983</td>\n",
              "      <td>1983-05-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19 Jun, 1971</td>\n",
              "      <td>1971-06-19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0           1\n",
              "0        26, SEP 2007  2007-09-26\n",
              "1  27 September, 1986  1986-09-27\n",
              "2            24.02.82  1982-02-24\n",
              "3         04-may-1983  1983-05-04\n",
              "4        19 Jun, 1971  1971-06-19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPukRirKHpq",
        "colab_type": "text"
      },
      "source": [
        "## Use TorchText to create train / val / test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMIoF0KEQXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-gxgiFJEm2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HUMAN = data.ReversibleField(init_token='<sot>',eos_token='<eot>',\n",
        "                             include_lengths=True,\n",
        "                             tokenize=list,\n",
        "                             lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoE8kGjaK4VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MACHINE = data.ReversibleField(init_token='<sot>',eos_token='<eot>',\n",
        "                               include_lengths=True,\n",
        "                               tokenize=list,\n",
        "                               lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NXIE2l5K6vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val, test = data.TabularDataset.splits(path=\"./date-normalization/\", \n",
        "                                              format='csv', \n",
        "                                              train='training.csv', \n",
        "                                              validation='validation.csv',\n",
        "                                              test='test.csv',\n",
        "                                              fields=[('human',HUMAN),('machine',MACHINE)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIW0OYKzLFxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HUMAN.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJOC0ap4LKKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MACHINE.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG6p6ssiLZC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex = train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74INwbYxLgdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "300f3777-2ee1-47a2-eb62-3e51a118941d"
      },
      "source": [
        "ex.human, ex.machine"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['3', '0', '/', '0', '7', '/', '7', '7'],\n",
              " ['1', '9', '7', '7', '-', '0', '7', '-', '3', '0'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw40gtucLNG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2fd13ce-3a2f-4a45-cbce-42609cb243ab"
      },
      "source": [
        "HUMAN.vocab.stoi['3']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Qz3-RgLrOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "134cfc66-ed9d-4f1a-a18d-8001ea5e3d16"
      },
      "source": [
        "HUMAN.vocab.itos[17]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD72FoDeLwEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "250423f4-b032-4353-e386-866a3949620b"
      },
      "source": [
        "HUMAN.process(['dec 7th 2019'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2],\n",
              "         [28],\n",
              "         [ 9],\n",
              "         [26],\n",
              "         [ 4],\n",
              "         [12],\n",
              "         [27],\n",
              "         [39],\n",
              "         [ 4],\n",
              "         [ 7],\n",
              "         [ 6],\n",
              "         [ 5],\n",
              "         [ 8],\n",
              "         [ 3]]), tensor([14]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-g9DR2TL78c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60e15db7-8e0c-4138-dd7b-5711258f2f11"
      },
      "source": [
        "HUMAN.reverse(HUMAN.process(['dec 7th 2019'])[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dec 7th 2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9GQkKh9PSYZ",
        "colab_type": "text"
      },
      "source": [
        "## RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G-Nz3a_PWwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HhBygCRXLH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # one hot encoding of characters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, input_size, _weight=torch.eye(input_size))\n",
        "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, bidirectional=True)\n",
        "    \n",
        "    def forward(self, input_batch, input_length, hidden=None):\n",
        "        embedded = self.embedding(input_batch) # T,B,F\n",
        "        packed = pack_padded_sequence(embedded, input_length)\n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        outputs, _ = pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNbUodUiXOwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, \n",
        "                 hidden_size, output_size, \n",
        "                 n_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # one hot encoding\n",
        "        self.embedding = nn.Embedding(input_size, input_size, _weight=torch.eye(input_size))\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(input_size, self.hidden_size, self.n_layers,\n",
        "                          dropout=(0 if n_layers==1 else self.dropout))\n",
        "        self.concat = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # this is run one word (one step) at a time\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "\n",
        "        output = self.out(rnn_output.squeeze(0))\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T97iRfsIZNa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/pytorch/text/issues/251\n",
        "def sequence_mask(length):\n",
        "    return (torch.arange(0, length.max())\n",
        "                .type_as(length)\n",
        "                .repeat(length.numel(), 1)\n",
        "                .lt(length.unsqueeze(1))).t()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrRvdWCUZOuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_nll_loss(inp, target, mask):\n",
        "    n_total = mask.sum()\n",
        "    cross_entropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
        "    loss = cross_entropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, n_total.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg5dogRmZWda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_train(input_variable, lengths, target_variable, mask, max_target_len, \n",
        "          encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, \n",
        "          batch_size, clip, max_length=12):\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    \n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[MACHINE.vocab.stoi['<sos>'] for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = False if np.random.rand() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, n_total = mask_nll_loss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, n_total = mask_nll_loss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYYBhi9GZkUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        input_seq = input_seq.to(device)\n",
        "        input_length = input_length.to(device)\n",
        "\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        # TODO: changed from decoder.n_layers to encoder.n_layers. Is this correct??\n",
        "        #import pdb; pdb.set_trace();\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        #decoder_input = torch.LongTensor([[MACHINE.vocab.stoi['<sos>'] for _ in range(batch_size)]])\n",
        "        decoder_input = torch.ones(1, input_seq.shape[-1], device=device, dtype=torch.long) * MACHINE.vocab.stoi['<sos>']\n",
        "        #print(f\"decoder_input {decoder_input.shape}, decoder_hidden {decoder_hidden.shape}\")\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = []\n",
        "        all_scores = []\n",
        "\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, \n",
        "                                                           decoder_hidden, \n",
        "                                                           encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            #print(f\"di = {decoder_input.shape}, ds = {decoder_scores.shape}\")\n",
        "            # Record token and score\n",
        "            all_tokens.append(decoder_input)\n",
        "            all_scores.append(decoder_scores)\n",
        "\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return torch.stack(all_tokens), torch.stack(all_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLlSifQHZv7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, batch):\n",
        "    with torch.no_grad():\n",
        "        encoder.train(mode=False)\n",
        "        decoder.train(mode=False)\n",
        "        gsd = GreedySearchDecoder(encoder, decoder)\n",
        "        all_tokens, all_scores  = gsd(batch.human[0], batch.human[1], 12)\n",
        "        correct = np.sum(np.all(batch.machine[0].data.numpy() == all_tokens.cpu().data.numpy(),\n",
        "                                axis=0))\n",
        "        batch_size = batch.machine[0].shape[-1]\n",
        "        return correct / batch_size, all_tokens, all_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTFgo86CZ2OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_iters(encoder, decoder, \n",
        "                encoder_optimizer, decoder_optimizer, \n",
        "                encoder_n_layers, decoder_n_layers, \n",
        "                n_epochs=50, \n",
        "                clip=50):\n",
        "\n",
        "    max_target_len = 12\n",
        "        \n",
        "    for epoch in range(n_epochs):\n",
        "        training_losses = []\n",
        "        train_iter, val_iter = get_train_val_iter()\n",
        "        \n",
        "        print(f\"Epoch = {epoch}\")\n",
        "        # Ensure dropout layers are in train mode\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        \n",
        "        for batch in train_iter:\n",
        "            input_variable, input_lengths = batch.human\n",
        "            target_variable, target_lengths = batch.machine\n",
        "            mask = sequence_mask(target_lengths)\n",
        "        \n",
        "\n",
        "            # Run a training iteration with batch\n",
        "            loss = do_train(input_variable, input_lengths, \n",
        "                         target_variable, mask, max_target_len, \n",
        "                         encoder, decoder, \n",
        "                         encoder_optimizer, decoder_optimizer, \n",
        "                         batch_size, \n",
        "                         clip)\n",
        "            training_losses.append(loss)\n",
        "            \n",
        "        val_accs = []\n",
        "        for batch in val_iter:\n",
        "            val_acc, _, _ = evaluate(encoder, decoder, batch)\n",
        "            val_accs.append(val_acc)\n",
        "            \n",
        "\n",
        "        print(f\"Mean Loss: {np.mean(training_losses)}, Mean Val Acc: {np.mean(val_accs)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6fct9yaSfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 25\n",
        "def get_train_val_iter():\n",
        "    train_iter, val_iter = data.BucketIterator.splits((train, val), \n",
        "                                                  batch_sizes=(batch_size,batch_size),\n",
        "                                                  sort_key=lambda x: len(x.human),\n",
        "                                                  sort_within_batch=True,\n",
        "                                                )\n",
        "    return iter(train_iter), iter(val_iter)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKo40tVZ5dQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "207514e6-9a6c-49df-8636-49d888b8994c"
      },
      "source": [
        "# Configure models\n",
        "hidden_size = 256\n",
        "encoder_n_layers = 1\n",
        "decoder_n_layers = 1\n",
        "dropout = 0.1\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "input_size = len(HUMAN.vocab)\n",
        "output_size = len(MACHINE.vocab)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(input_size, hidden_size)\n",
        "decoder = DecoderRNN(input_size, \n",
        "                     hidden_size, output_size, \n",
        "                     decoder_n_layers, dropout)\n",
        "\n",
        "gpu_is_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if gpu_is_available else \"cpu\")\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Model is ready to go!')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Model is ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hj42qdaGC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b7180ddd-5db7-434d-8b9b-edff287534d1"
      },
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 0.5 #0.5 #1.0\n",
        "learning_rate = 0.001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_epochs = 10\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "train_iters(encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "            encoder_n_layers, decoder_n_layers, n_epochs=n_epochs)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Epoch = 0\n",
            "Mean Loss: 0.15787426652136458, Mean Val Acc: 0.561\n",
            "Epoch = 1\n",
            "Mean Loss: 0.07634578954401502, Mean Val Acc: 0.635\n",
            "Epoch = 2\n",
            "Mean Loss: 0.02502268770417586, Mean Val Acc: 0.9730000000000001\n",
            "Epoch = 3\n",
            "Mean Loss: 0.00999346716466682, Mean Val Acc: 0.593\n",
            "Epoch = 4\n",
            "Mean Loss: 0.015643942681193495, Mean Val Acc: 1.0\n",
            "Epoch = 5\n",
            "Mean Loss: 0.00035623386887313107, Mean Val Acc: 1.0\n",
            "Epoch = 6\n",
            "Mean Loss: 0.00016742369553896555, Mean Val Acc: 1.0\n",
            "Epoch = 7\n",
            "Mean Loss: 0.00010463752979969006, Mean Val Acc: 1.0\n",
            "Epoch = 8\n",
            "Mean Loss: 7.134340589576663e-05, Mean Val Acc: 1.0\n",
            "Epoch = 9\n",
            "Mean Loss: 5.0772896396564426e-05, Mean Val Acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqiC4cFvgRVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(encoder, decoder, human):\n",
        "    with torch.no_grad():\n",
        "        encoder.train(mode=False)\n",
        "        decoder.train(mode=False)\n",
        "        gsd = GreedySearchDecoder(encoder, decoder)\n",
        "        all_tokens, all_scores = gsd(human[0], human[1], 12)\n",
        "        return all_tokens.cpu(), all_scores.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yskAnexogSKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_output_labels(input_text, output_text):\n",
        "    input_text = list(input_text)\n",
        "    input_text.insert(0,'<sot>')\n",
        "    input_text.append('<eot>')\n",
        "    \n",
        "    output_text = list(MACHINE.reverse(output_text)[0])\n",
        "    output_text.insert(0,'<sot>')\n",
        "    output_text.append('<eot>')\n",
        "        \n",
        "    return input_text, output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtVyC7r1gZpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_text = '7 dec, 19'\n",
        "output_text, scores = predict(encoder, decoder, HUMAN.process([input_text]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Kh3E5wgfKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c98e7f67-94cd-4d4c-f071-0c8a93d8e21b"
      },
      "source": [
        "input_label, output_label = get_input_output_labels(input_text, output_text)\n",
        "input_label, output_label"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<sot>', '7', ' ', 'd', 'e', 'c', ',', ' ', '1', '9', '<eot>'],\n",
              " ['<sot>', '2', '0', '1', '9', '-', '1', '2', '-', '0', '7', '<eot>'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CzNPN4BO2dQ",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "1. [Recurrent Neural Network with Attention](https://medium.com/datalogue/attention-in-keras-1892773a4f22])"
      ]
    }
  ]
}